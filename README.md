# KE5208-Sense-Making-and-Insight-Discovery-Project
Human Action Recognition using Multimodal Depth Camera and Inertial Sensors

Objective: Make sense of data from modality sensors for robust human action recognition.
Dataset: Human activity dataset
Sensor: RGB camera, depth camera, inertial sensor, microphone. 
Dataset: Dallas dataset, it contains 27 activities performed by 8 subjects (4 females and 4 males). Each subject repeated each activity 4 times. The dataset includes 861 data sequences, available at http://www.utdallas.edu/~cxc123730/UTD-MHAD.html
Task: Develop algorithm to perform activity classification (at least 3 categories) using either single sensor or fusion of multiple sensors. Note that the whole dataset contains 27 activity categories.

The dataset is named in the following format: aXX_sYY_tZZ_sensor.mat (such as a1_s1_t1_depth.mat)
XX: action id;
YY: subject id;
ZZ: time id;

